\subsubsection{LightGCN}
In \cite{LightGCN}, Xiangnan He et. al investigated the \textit{NCGF} method by exploring the effects that the nonlinear activation function and feature transformation had on the model's results.
They found that by removing these parts of the \textit{NGCF} model they were able to achieve better results, meaning that these parts negatively affected the training of the model.
Inspired by this, Xiangnan He et. al came up with a light but effective model of a graph convolutional network (GCN) with only the most essential parts used for collaborative filtering called \textit{LightGCN}.
For the experiments, we use the official TensorFlow based implementation of the paper available on \href{https://github.com/kuandeng/LightGCN}{GitHub}.
Like \textit{NGCF}, this method produces implicit ratings in the form of a top-$N$ list.
\\\\
The idea of a GCN is to learn the representation of nodes by smoothing features over the graph \cite{LightGCN}.
This is done by performing graph convolution where features of neighboring nodes are aggregated to give a new representation of the target node.
They propose an aggregation function called Light Graph Convolution (LGC) seen in \cref{eqn:lgc}.
\begin{align}\label{eqn:lgc}
    e_{u}^{(k+1)}=\sum_{i\in N_u}\frac{1}{\sqrt{\left | N_u \right |}\sqrt{\left | N_i \right |}}e_{i}^{(k)},\nonumber\\
    e_{i}^{(k+1)}=\sum_{u\in N_i}\frac{1}{\sqrt{\left | N_i \right |}\sqrt{\left | N_u \right |}}e_{u}^{(k)}
\end{align}
$e_{u}^{(k+1)}$ and $e_{i}^{(k+1)}$ are the embeddings at layer $k+1$ for a user $u$ and item $i$ respectively.
$N_u$ are the items that user $u$ has interacted with and $N_i$ are the users that have interacted with item $i$.
Note that this aggregation does not contain any nonlinear activation function nor any feature transformation, unlike NGCF's aggregation function described in \cref{eqn:ngcf-agg}.
$\frac{1}{\sqrt{\left | N_u \right |}\sqrt{\left | N_i \right |}}$ is a symmetric normalization term which makes sure that the embeddings do not scale with increasing graph convolution operations.
In LGC the target node is not included in the aggregation and only the neighboring nodes are used which means that it does not handle self-connections.
This is instead handled in a later operation where the layers are combined.
\\
The only trainable model parameters in \textit{LightGCN} are the embeddings at the 0-th layer which are $e_{u}^{(0)}$ for the users and $e_{i}^{(0)}$ for the items.
Embeddings at higher layers are computed using the LGC aggregation function. 
When the embeddings at the final layer have been computed, layers are combined through a summation to get a final presentation of the users and items.
\begin{align}\label{eqn:lgcn-layer-comb}
    e_{u} = \sum_{k=0}^K \alpha_k e_{u}^{(k)} \nonumber\\
    e_{i} = \sum_{k=0}^K \alpha_k e_{i}^{(k)}
\end{align}
In \cref{eqn:lgcn-layer-comb} we have $\alpha_k \geq 0$ which is a weight that indicates the importance of the embedding on layer $k$.
They found that setting this parameter to $1/(K+1)$ uniformly led to a good performance, but the parameter can be learned as a model parameter.
There are a few reasons why the layer combination is a good thing to do.
First of all, the embeddings become over-smoothed with an increasing number of layers, meaning that the embeddings slowly become indistinguishable when too much graph convolution is done.
This may be avoided by combining the embeddings at the different layers in the final step of the model.
Secondly, the embeddings at the different layers capture different features that would not be expressed if only the last layer's embeddings were used for the final embeddings.
For example, aggregation at the first layer captures the direct interactions between users and items, the second layer captures users that have an overlap in their item interactions, and so on.
Finally, when the embeddings at different layers are combined with a weighted sum, the effect of graph convolution with self-connections is captured.
\\
After the layer combination, the final embeddings can be used to make predictions with the model as seen in \cref{eqn:lgcn-pred}.
\begin{align}\label{eqn:lgcn-pred}
    \hat{y}_{ui} = e_U^{*^\textrm{T}} e_i^*
\end{align}
