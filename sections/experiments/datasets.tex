\subsection{Description of datasets}\label{subsec:desc-of-datasets}
Context-aware recommender systems consider various types of contextual information such as time, location, and social information when generating recommendations\cite{aggarwal2016recommender}.
In the following subsections, we will discuss specific datasets containing information that can be used for making context-aware recommenders.

\subsubsection{LDOS-CoMoDa}\label{subsub:desc-comoda}
The LDOS-CoMoDa\cite{COMODA2013} dataset is a context rich movie recommender dataset\cite{comoda}.
At the time of access, the dataset contains 121 users, 1,232 unique movies, and 2,296 ratings.
The dataset has a density of 1.54\%, representing the percentage of cells in the full user-item matrix that contain rating values
Most context variables are expressed for each rating.
The dataset contains the following context variables and their conditions:
\begin{itemize}
    \item Time: Morning, Afternoon, Evening, Night
    \item Daytype: Working day, Weekend, Holiday
    \item Season: Spring, Summer, Autumn, Winter
    \item Location:  Home, Public place, Friend's house
    \item Weather: Sunny / clear, Rainy, Stormy, Snowy, Cloudy
    \item Social: Alone, My partner, Friends, Colleagues, Parents, Public, My family
    \item EndEmo: Sad, Happy, Scared, Surprised, Angry, Disgusted, Neutral
    \item DominantEmo: Sad, Happy, Scared, Surprised, Angry, Disgusted, Neutral
    \item Mood: Positive, Neutral, Negative
    \item Physical: Healthy, Ill
    \item Decision: User decided which movie to watch, User was given a movie
    \item Interaction: First interaction with a movie, N-th interaction with a movie
\end{itemize}
EndEmo and DominantEmo relate to the emotional state of the user during the consumption stage.
DominantEmo defines the emotional state that was dominant during the consumption of the movie, whereas EndEmo defines the emotional state of the user at the end of the movie\cite{COMODA2013}.\\
In \cite{COMODA2013}, Ante Odić et. al indicate that, on other datasets where the only context that could be derived were based on timestamps, many users would leave these ratings in a relatively short period of time, making them not representative of the contextual situation of the user at the time of consumption.
The paper thus proposes the LDOS-CoMoDa dataset containing potential contextual information from the consumption stage, gathered through ratings and an accompanying questionnaire.
\\\\
In \cite{COMODA2013}, they employed a relevant-context-detection procedure to determine which of these contextual variables were in fact relevant.
This was done through statistical hypothesis testing with a power analysis, where independence was tested between each contextual variable and the ratings.
The null hypothesis of the test stated that the two variables were independent, whereas the alternative hypothesis stated that they were dependent.
If the null hypothesis was rejected, a conclusion was drawn that the contextual variable and the rating were dependent and thus the contextual information was relevant.
They employed a significance level of $\alpha = 0.05$ for the test.
\\\\
The testing determined that six of the variables proved to be relevant - EndEmo, DominantEmo, Mood, Physical, Decision and Interaction.
Location and Daytype could not be declared irrelevant, and Time, Season, Weather and Social were rejected as irrelevant contextual information.
Generally, the paper finds that the variables detected as relevant perform better than the irrelevant ones, apart from the Mood variable, which performed worse than a variable deemed irrelevant.
This means that, with the exception of the variable Mood, the paper finds that the contextual variables detected as relevant tend to perform better than the uncontextualized models, while the contextual variables detected as irrelevant tend to perform worse than the uncontextualized models, if there are enough ratings per each context variable value during training.
The anomaly regarding Mood can be explained by an isolated case of high sparsity in the negative condition of the variable for the dataset.

\subsubsection{MovieLens}
The MovieLens\cite{movielens} datasets are datasets provided by GroupLens research from the MovieLens web site.
These datasets were collected over various periods of time, and are available in different sizes.
The papers that were investigated made use of both the 1M dataset and the 100K stable benchmark datasets.
The 1M dataset contains 1,000,209 ratings of 3,706 movies made by 6,040 users with a density of 4.47\% \cite{MovieLens2015}.
The 100K Dataset contains 100,000 ratings of 1,682 movies made by 943 users with a density of 6.30\% \cite{MovieLens2015}.
Each user in both datasets has rated at least 20 movies.
The datasets do not contain specific contextual information, but it is possible to derive a time context variable from the timestamps provided along the ratings.

\subsubsection{Frappé}
Frappé\cite{baltrunas2015frappe} is a mobile app recommender providing context-aware mobile app recommendations by means of a tensor factorization approach based on implicit feedback data.
Frappé was deployed on Android, leading to a context-aware app usage data set.
Frappé collected implicit data on the following relevant context dimensions: time of day, weekday, whether or not it is weekend, at home or at work, weather, country, cost, and city. 
The dataset consists of 96,203 entries by 957 users for 4,082 apps, and a density of 2.46\%.
Implicit data in this context means that the data is gathered without the user explicitly giving a rating, unlike the other datasets where a user has explicitly given a rating from 1-5 before an interaction is registered.

\subsubsection{InCarMusic}
InCarMusic\cite{InCarMusic2011} is a mobile Android application offering music recommendations for the passengers of cars.
In order to provide these recommendations, \cite{InCarMusic2011} collected the user's assessment of the effect of context on their music preferences, as well as had them enter ratings for tracks assuming certain contextual conditions held.
\cite{InCarMusic2011} identified the following contextual variables as potentially relevant: driving style, road type, landscape, sleepiness, traffic conditions, mood, weather, and natural phenomena.
The data collection was carried out in two phases: one with an aim of determining the contextual factors that are more influential in changing the propensity of the user to listen to music of different genres, and another interested in individual tracks and their ratings, examining the case without considering any contextual conditions, and the case under the assumption that a certain contextual condition holds.
Ultimately, this resulted in a dataset consisting of 4,012 ratings, given by 42 different users on 139 songs for a density of 68.72\%.
An issue with this dataset is that each entry only has data on one contextual variable, and the rest are unknown.

\subsubsection{DePaul}
The DePaulMovie\cite{DePaulData} dataset is another alternative.
This dataset has 5,029 ratings from 1-5, given by 97 users on 79 movies within three different context variables: time, location, and companion\cite{DePaulData}.
The dataset has a density of 65.63\%.
The contextual variables distinguish between weekday or weekend, whether or not the movie was watched at home or at the cinema, and finally if it was watched alone, with family or with partner.

\subsubsection{Yahoo! Movies}
The Yahoo! dataset\cite{yahoo-movie} contains a dataset of $7,642$ users, $11,915$ movies and $211,231$ ratings, representing the most sparse dataset being investigated with a density of 0.23\%.
All users have rated at least one item, and all items are rated by at least one user.
In terms of contextual information, the dataset does not provide any contextual information for the recorded ratings.
It instead provides information such as year of birth for the user, gender and age group which is discretized as either child, adult or elder.
We opted to try and use this information as context for the context-aware methods.
The movies are also linked to $32$ types of side information, such as synopsis, running time, release date, list of genres, and list of crew members.
This information can be used for methods that utilize item side-information, such as knowledge-graph-based methods.

\subsection{Evaluation protocols}\label{sec:evaluationmetrics}
Ricci et al.\cite{RecommenderHandbook2015} define a set of properties for RS for evaluation: user preference, prediction accuracy, coverage, confidence, trust, novelty, serendipity, diversity, utility, risk, robustness, privacy, adaptivity, and scalability.
Each property is suited to certain types of tests, and different metrics are used for evaluating these properties.
Some, such as user preference and trust are more suitable for testing through user studies.
Properties relating to algorithmic effectiveness such as prediction accuracy, coverage and confidence are more suitable for offline experiments, while properties that relate to active use of the RS, such as serendipity, are suitable for online studies where real users interact with the system.
For the purposes of this paper, offline evaluation metrics are the focus as the experiments carried out are not suited for user studies and there is no available environment in which to carry out online testing.
\begin{table*}[!htp]\centering
    \caption{Metrics used.}\label{tab:metricsused}
    \scriptsize
    \begin{tabular}{lccccccccc}\toprule
    &\textbf{MSE} &\textbf{RMSE} &\textbf{MAE} &\textbf{Precision} &\textbf{Recall} &\textbf{NDCG} &\textbf{F1} &\textbf{Hit rate} &\textbf{MAP} \\\cmidrule{2-10}
    \textbf{KGAT\cite{KGAT}} & & & & & x & x & & & \\\cmidrule{1-10}
    \textbf{IS-UserBased\cite{GraphBasedCollaborativePaper}} & & & & x & & & & & x \\\cmidrule{1-10}
    \textbf{LightGCN\cite{LightGCN}} & & & & & x & x & & & \\\cmidrule{1-10}
    \textbf{NGCF\cite{NGCF} } & & & & & x & x & & & \\\cmidrule{1-10}
    \textbf{DeepWalk\cite{DeepWalk}} & & & & Implicit & Implicit & & x & & \\\cmidrule{1-10}
    \textbf{NMF\cite{NMF} } & x & & & & & & & & \\\cmidrule{1-10}
    \textbf{CAMF(I)\cite{baltrunasCAMF} } & & & x & & & & & & \\\cmidrule{1-10}
    \textbf{SVD\cite{standardMF} } & x & x & & & & & & & \\\cmidrule{1-10}
    \textbf{SVD++\cite{svd++} } & & x & & & & & & & \\\cmidrule{1-10}
    \bottomrule

    \end{tabular}
\end{table*}
\begin{table}[]\centering
    \caption{Methods used.}\label{tab:methodsused}
    \scriptsize
    \begin{tabular}{ll}\toprule
        &\textbf{Method used}\\\cmidrule{1-2}
        \textbf{KGAT\cite{KGAT}} & Training, validation, test (70\%, 10\%, 20\%) \\\cmidrule{1-2}
        \textbf{IS-UserBased\cite{GraphBasedCollaborativePaper}} & 10-fold cross validation  \\\cmidrule{1-2}
        \textbf{LightGCN\cite{LightGCN}} & From original authors (including splits) \\\cmidrule{1-2}
        \textbf{NGCF\cite{NGCF} } & Training, validation, test (70\%, 10\%, 20\%) \\\cmidrule{1-2}
        \textbf{DeepWalk\cite{DeepWalk}} & 10-fold cross validation \\\cmidrule{1-2}
        \textbf{NMF\cite{NMF} } & 80/20 split and 5-fold cross validation \\\cmidrule{1-2}
        \textbf{CAMF(I)\cite{baltrunasCAMF} } & 25 splits of training, test (90\%, 10\%) \\\cmidrule{1-2}
        \textbf{SVD\cite{standardMF} } & Netflix prize testing \\\cmidrule{1-2}
        \textbf{SVD++\cite{svd++} } & Test, validation (both 1.4 million ratings) \\\cmidrule{1-2}
    \bottomrule
    \end{tabular}
\end{table}
\\\\
\autoref{tab:metricsused} shows the metrics used for evaluation for the relevant methods, while \autoref{tab:methodsused} examines each paper for the evaluation protocol.
It is evident that most of these papers focus on offline evaluation, investigating metrics related to algorithmic precision and efficiency, such as root-mean-square error (RMSE), mean absolute error (MAE) and F1-measure.
These metrics are useful for two important problems associated with recommender systems: rating prediction and top-$N$ recommendation\cite{RecommenderHandbook2015}.
\\
The rating prediction problem concerns itself with predicting the rating that a user $u$ will give an unrated item $i$ denoted as $r_{ui}$, which is often defined as learning a function $f : U \times I \rightarrow S$, that predicts the rating $f(u, i)$ of a user $u$ for a new item $i$, where $U$ the set of users, $I$ is the set of items, and $S$ is the set of possible values for a rating.
Typically, ratings in a set of ratings $R$ are divided into a training set $R_{train}$ used to learn $f$, and a test set $R_{test}$ used for evaluation prediction accuracy, which is usually done through MAE and RMSE.
These metrics compare the predicted ratings to the observed values in the test set, meaning they depend on the magnitude of the errors made.
\autoref{tab:rmsevsmae} illustrates two potential test sets along with fictive predicted errors for the items they contain.
The main difference between the two metrics is that RMSE penalizes larger errors more harshly compared to MAE.
RMSE would prefer test set 1 in \autoref{tab:rmsevsmae}, even though it has an aggregated error of 6 compared to test set 2 with an error of 4, due to the larger error of 4 on item 3.
The RMSE for the first test set would be 1.73 versus 2 on the second set.
MAE would prefer test set 2 with a score of 1 versus 1.5 on the first.
\begin{table}[]\centering
    \caption{Example of two test sets and the prediction errors on the items.}\label{tab:rmsevsmae}
    \scriptsize
    \begin{tabular}{ccccc}\toprule
        &\textbf{Item 1} & \textbf{Item 2} & \textbf{Item 3} & \textbf{Item 4}\\\cmidrule{1-5}
        \textbf{Test set 1} & 2 & 0 & 2 & 2 \\\cmidrule{1-5}
        \textbf{Test set 2} & 0 & 0 & 4 & 0  \\\cmidrule{1-5}
    \bottomrule
    \end{tabular}
\end{table}
MAE is calculated according to \autoref{eqn:MAE}.
\begin{equation}
    \label{eqn:MAE}
    MAE(f) = \frac{1}{|R_{test}|} \sum\limits_{r_{ui} \epsilon R_{test}} |f(u,i)-r_{ui}|
\end{equation}
RMSE is calculated according to \autoref{eqn:RMSE}.
\begin{equation}
    \label{eqn:RMSE}
    RMSE(f) = \sqrt{\frac{1}{R_{test}} \sum\limits_{r_{ui} \epsilon R_{test}} (f(u, i) - r_{ui})^2}
\end{equation}
The top-$N$ recommendation problem is the task of recommending a list $L(u)$ to an active user $u$ containing $N$ items to likely be of interest.
Evaluating the quality of such method can be done by splitting $I$ into a training set $I_{train}$ used to learn $L$ and a test set $I_{test}$ for each user $u$.
Let $T(u) \subset I_u \cap I_{test}$, where $I_u$ is the subset of items that have been rated by user $u$, then precision can be calculated according to \autoref{eqn:precision}, and recall according to \autoref{eqn:recall}.
Precision defines the proportion of relevant instances among the predicted relevant items, while recall is the proportion of true positives that were correctly predicted.
A trade off between these measures is expected.
Allowing longer recommendation lists improves recall and is likely to reduce precision\cite{RecommenderHandbook2015}.
\begin{equation}
    \label{eqn:precision}
    Precision@N(L) = \frac{1}{|U|} \sum\limits_{u \epsilon U}\frac{|L(u) \cap T(u)|}{|L(u)|}
\end{equation}
\begin{equation}
    \label{eqn:recall}
    Recall@N(L) = \frac{1}{|U|} \sum\limits_{u \epsilon U} \frac{|L(u) \cap T(u)|}{|T(u)|}
\end{equation}
The F1-measure is also employed by some of the papers.
This measure summarizes precision and recall into a single number as defined in \autoref{eqn:f1}.
It is useful for the purposes of comparison rather than employing either recall or precision by itself, taking the trade off into account.
\begin{equation}
    \label{eqn:f1}
    F1@N = \frac{2*Precision@N(L)*Recall@N(L)}{Precision@N(L)+Recall@N(L)}
\end{equation}
To give an example of the calculation of Precision(L), Recall(L) and F1-measure, a top-$N$ set and a relevant item set is defined in \autoref{eqn:item-lists} to be used for the calculation.
\begin{align}
    T(u) = \{item1, \: item3, \: item4, \: item5,\nonumber \\
     item6\} \nonumber \\
    L(u) = \{item1, \: item2, \: item3, \: item4\}\label{eqn:item-lists}
\end{align}
The intersection between the top-$N$ set $L(u)$ and the relevant items $T(u)$ is a set containing three items $\{item1, \: item3, \: item4\}$.
The lengths of the $L(u)$ and $T(u)$ sets in this example are four and five respectively.
These values are then plotted into the equations to get the precision and recall values.
Note that for this example there is only a single user.
\begin{align*}
    Precision@N(L) = \frac{1}{1} \sum\limits_{u \epsilon 1}\frac{3}{4} = 0.75\\
    Recall@N(L) = \frac{1}{1} \sum\limits_{u \epsilon 1} \frac{3}{5} = 0.60
\end{align*}
These values can then be used to calculate the F1-score according to \autoref{eqn:f1}.
\begin{align*}
    F1@N = \frac{2*0.75*0.60}{0.75+0.60} = 0.667
\end{align*}
Other metrics are derived from these basic information retrieval metrics, with \textit{Mean Average Precision} (MAP) representing a popular metric\cite{ChoosingMetricsEvaluation}.
To calculate MAP, the first step is to calculate the \textit{Average Precision} (AP).
This measure takes each relevant item and calculates precision in relation to the position of the item in the recommendation set.
This is defined in \autoref{eq:averageprecision}, where $Precision(n)$ is the precision at the $n^{th}$ item and $Relevant(n)$ is an indicator that the item was relevant, taking the value of either 1 if relevant, or 0 if not.
The denominator used for the calculation is normalized to be the smaller of either the length of the list, the $N$ value, or the number of relevant items in case the user has less than $N$ observed ratings.
A point to note about AP is that it rewards recommendation lists with relevant items appearing at the front of the list, leading to a higher AP value in comparison to lists where relevant items appear nearer the $N$ value. 
\begin{equation}
    \label{eq:averageprecision}
    AP@N = \frac{\sum\limits_{n=1}^N (Precision@N(n) \times Relevant(n))}{min(|Relevant(N)|,\:N)}
\end{equation}
Once the AP is calculated for a single user $u$, MAP@N for a recommender system is defined through \autoref{eq:map}, where $U$ is the set of all users: 
\begin{equation}
    \label{eq:map}
    MAP@N = \frac{\sum\limits_{u=1}^{|U|} AP_u}{|U|}
\end{equation}
To show an example of how to calculate AP and MAP, the sets defined in \autoref{eqn:item-lists} are used once again.
Here we have that $N$ is 4 as that is the length of the recommendation list $L(u)$.
\begin{align}
    AP@N = \frac{(\frac{1}{1}*1+\frac{1}{2}*0+\frac{2}{3}*1+\frac{3}{4}*1)}{4} = 0.604 \label{eqn:ap-example}
\end{align}
The Precision@N from $n=1$ to $N$, multiplied by 1 if item $n$ is relevant and 0 if not, are all summed together in the numerator as seen in \autoref{eqn:ap-example}.
The resulting value is then divided by the minimum of either $N$ or the length of the relevant item set for the user, which in this case is equal to $N$.
To find the MAP value, the AP values are simply summed together for each user in $U$ and divided by $|U|$.
In this example we only have a single user, so the MAP is the same as the AP.
\\\\
Finally, some of the papers use the Discounted Cumulative Gain (DCG) metric or the normalized version NDCG.
Assuming each user $u$ has a gain $g_{ui}$ from being recommended item $i$, then the average DCG for a list of $J$ items is defined in \autoref{eqn:dcg}, where $i_j$ is the item at position $j$ in the list, and the logarithm base is a free parameter, where $2$ is most commonly used to ensure all positions are discounted.
This metric also rewards lists that frontload relevant items.
\begin{equation}
    \label{eqn:dcg}
    DCG@N = \frac{1}{|U|} \sum\limits_{u=1}^{|U|} \sum\limits_{l = 1}^{|L|} \frac{g_{ui_l}}{log_b (l+1)}
\end{equation}
NDCG is defined in \autoref{eqn:ndcg}, where $DCG*$ is the ideal DCG, which is defined by sorting the DCG vector such that the most relevant items appear in the start of the list\cite{dcgpaper}, resulting in the biggest possible DCG value.
\begin{equation}
    \label{eqn:ndcg}
    NDCG@N = \frac{DCG}{DCG*}
\end{equation}
\\\\
Once again the sets in \autoref{eqn:item-lists} can be used to give an example of how to calculate DCG and NDCG.
In the example, the gain $g_{ui}$ is set to be $1$ when a relevant item is being recommended, and $log_2$ is used.
Again, there is only a single user which makes the calculation quite simple.
\begin{align*}
    DCG@N = \frac{1}{1} &\left (\frac{1}{log_2 (1+1)} + \frac{0}{log_2 (2+1)} \right.\\
    & \left.+ \frac{1}{log_2 (3+1)} + \frac{1}{log_2 (4+1)}\right) \\
    & = 1.93
\end{align*}
In the example the first recommended item is relevant, the second is not, and the third and fourth are relevant.
Then the DCG* is calculated which is done by sorting the recommendation set such that the most relevant items come first, resulting in the largest possible DCG value.
\begin{align*}
    DCG*@N = \frac{1}{1} &\left (\frac{1}{log_2 (1+1)} + \frac{1}{log_2 (2+1)} \right.\\
    & \left.+ \frac{1}{log_2 (3+1)} + \frac{0}{log_2 (4+1)}\right) \\
    & = 2.13
\end{align*}
Now the NDCG can be found by dividing the DCG value with the DCG* value.
\begin{align*}
    NDCG@N = \frac{1.93}{2.13} = 0.91
\end{align*}
Now that the metrics have been examined, we will look into splitting the data.
The papers examined throughout this research employ cross validation across a number of folds, in a range of 5-10 folds.
Most of them conform to splitting the data into an 80\%/20\% split of training and test data, as seen on \autoref{tab:methodsused}.
Another interesting aspect to look into for the various papers that have been researched is which metrics they use for comparisons.
The results of this can be seen on \autoref{tab:metricsused}.
\\\\
A summary of the details of the datasets can be seen in \autoref{tab:datasetstats}.
The most interesting datasets for context-aware recommendation and side-information are LDOS-CoMoDa due to the amount of contextual values, the Yahoo! dataset due to the size and inclusion of side-information, the MovieLens datasets due to their size and ability to extract a temporal context, as well as Frappé due to its size.
Most of the examined papers perform offline evaluation, mainly employing Recall@N and NDCG@N to evaluate the algorithms, and cross-validate on the sets when testing.
\begin{table*}[]\centering
    \caption{A final summary of the datasets.}\label{tab:datasetstats}
    \scriptsize
    \begin{tabular}{cccccc}\toprule
        &\textbf{\#Ratings} & \textbf{\#Items} & \textbf{\#Users} & \textbf{\#Context variables} & \textbf{Density}\\\cmidrule{1-6}
        \textbf{LDOS-CoMoDa} & 2,296 & 1,232 & 121 & 12 & 1.54\% \\\cmidrule{1-6}
        \textbf{MovieLens 1M} & 1,000,209 & 3,706 & 6,040 & 1 & 4.47\% \\\cmidrule{1-6}
        \textbf{MovieLens 100K} & 100,000 & 1,682 & 943 & 1 & 6.3\% \\\cmidrule{1-6}
        \textbf{Frappé} & 96,203 & 957 & 4,082 & 8 & 2.46\% \\\cmidrule{1-6}
        \textbf{InCarMusic} & 4,012 & 139 & 42 & 8 & 68.72\% \\\cmidrule{1-6}
        \textbf{DePaulMovie} & 5,029 & 79 & 97 & 3 & 65.63\% \\\cmidrule{1-6}
        \textbf{Yahoo!} & 211,231 & 11,915 & 7,642 & 0 & 0.23\% \\\cmidrule{1-6}
    \bottomrule
    \end{tabular}
\end{table*}
