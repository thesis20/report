\subsection{Description of datasets}\label{subsec:desc-of-datasets}
Context-aware recommender systems consider various types of contextual information such as time, location, and social information when generating recommendations.
They have generally been observed to greatly improve the effectiveness of recommendation processes \cite{aggarwal2016recommender}.
To establish the usefulness of adding context to recommender systems we will conduct an investigation into recent papers relating to the topic examining the experimental results of different proposed methods.
We will investigate the kinds of context data used in existing papers, how this context data was used, and how it was evaluated.
\autoref{tab:paperdatasets} shows an overview of different papers relating to the topic of context-aware recommendations and the datasets used for evaluation.
In the following subsections we will discuss the specific datasets and how they were used.

\subsubsection{LDOS-CoMoDa}
The LDOS-CoMoDa dataset is a context rich movie recommender dataset \cite{comoda}.
At the time of access, the dataset contains 121 users, 1232 unique movies, and 2296 ratings.
Most context variables are expressed for each rating.
The dataset contains the following context variables and their conditions:
\begin{itemize}
    \item Time: Morning, Afternoon, Evening, Night
    \item Daytype: Working day, Weekend, Holiday
    \item Season: Spring, Summer, Autumn, Winter
    \item Location:  Home, Public place, Friend's house
    \item Weather: Sunny / clear, Rainy, Stormy, Snowy, Cloudy
    \item Social: Alone, My partner, Friends, Colleagues, Parents, Public, My family
    \item EndEmo: Sad, Happy, Scared, Surprised, Angry, Disgusted, Neutral
    \item DominantEmo: Sad, Happy, Scared, Surprised, Angry, Disgusted, Neutral
    \item Mood: Positive, Neutral, Negative
    \item Physical: Healthy, Ill
    \item Decision: User decided which movie to watch, User was given a movie
    \item Interaction: First interaction with a movie, N-th interaction with a movie
\end{itemize}
\textit{EndEmo} and \textit{DominantEmo} relate to the emotional state of the user during the consumption stage.
\textit{DominantEmo} defines the emotional state that was dominant during the consumption of the movie, whereas \textit{EndEmo} defines the emotional state of the user at the end of the movie \cite{COMODA2013}.
\cite{COMODA2013} indicates that, on other datasets where the only context that could be derived were based on timestamps, many users would leave these ratings in a relatively short period of time, making them not representative of the contextual situation of the user at the time of consumption.
The paper thus proposes the LDOS-CoMoDa dataset containing potential contextual information from the consumption stage, gathered through ratings and an accompanying questionnaire.
\\\\
\cite{COMODA2013} employed a relevant-context-detection procedure to determine which of these contextual variables were in fact relevant.
This was done through statistical hypothesis testing with a power analysis, where independence was tested between each contextual variable and the ratings.
The null hypothesis of the test stated that the two variables were independent, whereas the alternative hypothesis stated that they were dependent.
If the null hypothesis was rejected, a conclusion was drawn that the contextual variable and the rating were dependent and thus the contextual information was relevant.
They employed a significance level of $\alpha = 0.05$ for the test.
\\\\
The testing determined that six of the variables proved to be relevant, making them contextual - \textit{EndEmo, DominantEmo, Mood, Physical, Decision} and \textit{Interaction}.
\textit{Location} and \textit{Daytype} could not be declared irrelevant, and \textit{Time, Season, Weather} and \textit{Social} were rejected as irrelevant contextual information.
Generally, the paper finds that the variables detected as relevant perform better than the irrelevant ones, apart from the \textit{Mood} variable, which performed worse than a variable deemed irrelevant.
This means that, with the exception of the variable \textit{Mood}, the paper finds that the contextual variables detected as relevant tend to perform better than the uncontextualized models, while the contextual variables detected as irrelevant tend to perform worse than the uncontextualized models, if there are enough ratings per each context variable value during training.
The anomaly regarding \textit{Mood} can be explained by an insolated case of high sparsity in the negative condition for the dataset.

\subsubsection{MovieLens}
The MovieLens datasets are datasets provided by GroupLens research from the MovieLens web site.
These datasets were collected over various periods of time, and are available in different sizes \cite{movielens}.
The papers that were investigated made use of both the 1M dataset and the 100K stable benchmark datasets.
The 1M dataset contains 1,000,209 ratings of 3706 movies made by 6040 users with a density of 4.47\%, representing the percentage of cells in the full user-item matrix that contain rating values \cite{MovieLens2015}.
The 100K Dataset contains 100,000 ratings of 1682 movies made by 943 users with a density of 6.30\% \cite{MovieLens2015}.
Each user in both datasets has rated at least 20 movies.
The datasets do not contain specific contextual information, but it is possibly to derive a time context dimension from the timestamps provided along the ratings.

\subsubsection{Frappé}
Frappé is a mobile app recommender providing context-aware mobile app recommendations by means of a tensor factorization approach based on implicit feedback data\cite{baltrunas2015frappe}.
Frappé was deployed on Android, leading to a context-aware app usage data set.
Frappé collected implicit data on the following relevant context dimensions: \textit{time of day, weekday, whether or not it is weekend, at home or at work, weather, country, cost} and \textit{city}. 
The dataset consists of 96,203 entries by 957 users for 4082 apps.
Implicit data in this context means that the data is gathered without the user explicitly giving a rating, unlike the other datasets where a user has explicitly given a rating from 1-5 before an interaction is registered.

\subsubsection{InCarMusic}
InCarMusic is a mobile Android application offering music recommendations for the passengers of cars.
In order to provide these recommendations, \cite{InCarMusic2011} collected the user's assessment of the effect of context on their music preferences, as well as had them enter ratings for tracks assuming certain contextual conditions held.
\cite{InCarMusic2011} identified the following contextual variables as potentially relevant: driving style, road type, landscape, sleepiness, traffic conditions, mood, weather, natural phenomena.
The data collection was carried out in two phases: one with an aim of determining the contextual factors that are more influential in changing the propensity of the user to listen to music of different genres, and another interested in individual tracks and their ratings, examining the case without considering any contextual conditions, and the case under the assumption that a certain contextual condition holds.
Ultimately, this resulted in a dataset consisting of 4012 ratings, given by 42 different users on 139 songs.
An issue with this dataset is that each entry only has data on one contextual dimension, and the rest are unknown.

\subsubsection{DePaul}
The \textit{DePaulMovie} dataset is another alternative.
This dataset has 5,029 ratings from 1-5, given by 97 users on 79 movies within three different context dimensions: \textit{time, location} and \textit{companion}\cite{DePaulData}.
The contextual dimensions distinguish between weekday or weekend, whether or not the movie was watched at home or at the cinema, and finally if it was watched alone, with family or with partner.

\subsubsection{Yahoo! Movies}
The \textit{Yahoo!} dataset contains a dataset of $7,642$ users, $11,915$ movies and $211,231$ ratings.
All users have rated at least one item, and all items are rated by at least one user.
In terms of contextual information, the dataset provides \textit{year of birth} for the user, \textit{gender} and \textit{age group} which is discretized as either child, adult or elder.
The movies are also linked to $32$ types of content information, such as \textit{synopsis}, \textit{running time}, \textit{release date}, \textit{list of genres} and \textit{list of crew members}.
This information can be used for methods that utilize knowledge, such as knowledge-graph-based methods.

\subsection{Evaluation protocols}\label{sec:evaluationmetrics}
Ricci et al. \cite{RecommenderHandbook2015} defines a set of properties for recommender systems for evaluation: \textit{user preference, prediction accuracy, coverage, confidence, trust, novelty, serendipity, diversity, utility, risk, robustness, privacy, adaptivity} and \textit{scalability}.
Each property is suited to certain types of tests, and different metrics are used for evaluating these properties.
Some, such as user preference and trust are more suitable for testing through user studies.
Properties relating to algorithmic effectiveness such as prediction accuracy, coverage and confidence are more suitable for offline experiments, while properties that relate to active use of the recommender system, such as serendipity, are suitable for online studies where real users interact with the system.
\\\\
\autoref{tab:metricsused} shows the metrics used for evaluation for the relevant methods, while \autoref{tab:methodsused} examines each paper for the evaluation protocol.
It is evident that most of these papers focus on offline evaluation, investigating metrics related to algorithmic precision and efficiency, such as root-mean-square error(RMSE), mean absolute error(MAE) and F1-measure.
These metrics are useful for two important problems associated with recommender systems: rating prediction and top-N recommendation\cite{RecommenderHandbook2015}.
\\\\
The rating prediction problem concerns itself with predicting the rating that a user $u$ will give an unrated item $i$ denoted as $r_{ui}$, which is often defined as learning a function $f : U \times I \rightarrow S$, that predicts the rating $f(u, i)$ of a user $u$ for a new item $i$, where $U$ the set of users, $I$ is the set of items, and $S$ is the set of possible values for a rating.
Typically, ratings in a set of ratings $R$ are divided into a training set $R_{train}$ used to learn $f$, and a test set $R_{test}$ used for evaluation prediction accuracy, which is usually done through MAE and RMSE.
These metrics compare the predicted ratings to the observed values in the test set, meaning they depend on the magnitude of the errors made.
\autoref{tab:rmsevsmae} illustrates two potential test sets along with fictive predicted errors for the items they contain.
The main difference between the two metrics is that RMSE penalizes larger errors more harshly compared to MAE.
RMSE would prefer test set 1 in \autoref{tab:rmsevsmae}, even though it has an aggregated error of 6 compared to test set 2 with an error of 4, due to the larger error of 4 on item 3.
The RMSE for the first test set would be 3.46 versus 4 on the second set.
MAE would prefer test set 2 with a score of 1 versus 1.5 on the first.
\begin{table}[]
    \begin{tabular}{|l|l|l|l|l|}
    \hline
               & Item 1 & Item 2 & Item 3 & Item 4 \\ \hline
    Test set 1 & 2      & 0      & 2      & 2      \\ \hline
    Test set 2 & 0      & 0      & 4      & 0      \\ \hline
    \end{tabular}
    \caption{Example of two test sets and the predicted errors on the items}
    \label{tab:rmsevsmae}
\end{table}
MAE is calculated according to \autoref{eqn:MAE}.
\begin{equation}
    \label{eqn:MAE}
    MAE(f) = \frac{1}{|R_{test}|} \sum\limits_{r_{ui} \epsilon R_{test}} |f(u,i)-r_{ui}|
\end{equation}
RMSE is calculated according to \autoref{eqn:RMSE}.
\begin{equation}
    \label{eqn:RMSE}
    RMSE(f) = \sqrt{\frac{1}{R_{test}} \sum\limits_{r_{ui} \epsilon R_{test}} (f(u, i) - r_{ui})^2}
\end{equation}
The top-N recommendation problem is the task of recommending a list $L(u_a)$ to an active user $u_a$ containing $N$ items to likely be of interest.
Evaluating the quality of such method can be done by splitting $I$ into a training set $I_{train}$ used to learn $L$ and a test set $I_{test}$.
Let $T(u) \subset I_u \cap I_{test}$, where $I_u$ is the subset of items that have been rated by user $u$, then precision can be calculated according to \autoref{eqn:precision}, and recall according to \autoref{eqn:recall}.
Precision defines the proportion of relevant instances among the predicted relevant items, while recall is the proportion of true positives that were correctly predicted.
A trade off between these measures is expected.
Allowing longer recommendation lists improves recall and is likely to reduce precision \cite{RecommenderHandbook2015}.
\begin{equation}
    \label{eqn:precision}
    Precision(L) = \frac{1}{|U|} \sum\limits_{u \epsilon U} |L(u) \cap T(u)| / |L(u)|
\end{equation}
\begin{equation}
    \label{eqn:recall}
    Recall(L) = \frac{1}{|U|} \sum\limits_{u \epsilon U} |L(u) \cap T(u)| / |T(u)|
\end{equation}
The F1-measure also employed by some of the papers summarizes precision and recall into a single number, the harmonic mean of precision and recall, defined by \autoref{eqn:f1}.
This is useful for the purposes of comparison rather than employing either recall or precision by itself, taking the trade off into account.
\begin{equation}
    \label{eqn:f1}
    F1-measure = \frac{2*precision*recall}{precision+recall}
\end{equation}
Other metrics are derived from these basic information retrieval metrics, with \textit{Mean Average Precision} (MAP) representing a popular metric \cite{ChoosingMetricsEvaluation}.
To calculate MAP, the first step is to calculate the \textit{Average Precision} (AP).
This measure takes each relevant item and calculates precision in relation to the position of the item in the recommendation set.
This is defined in \autoref{eq:averageprecision}, where $P(r)$ is the precision at the $r^{th}$ item and $rel(r)$ is an indicator that the item was relevant, taking the value of either 1 if relevant, or 0 if not.
The denominator used for the calculation is normalized to be the smaller of either the length of the list, the $N$ value, or the number of relevant items in case the user has less than $N$ observed ratings.
A point to note about AP is that it rewards recommendation lists with relevant items appearing at the front of the list, leading to a higher AP value in comparison to lists where relevant items appear nearer the N value. 
\begin{equation}
    \label{eq:averageprecision}
    AP = \frac{\sum\limits_{r=1}^N (P(r) \times rel(r))}{minimum(number \: of \: relevant \: documents, N)}
\end{equation}
Once the AP is calculated for a single user $u$, MAP@N for a recommender system is defined through \autoref{eq:map}, where $U$ is the set of all users: 
\begin{equation}
    \label{eq:map}
    MAP = \frac{\sum\limits_{u=1}^U AP_u}{U}
\end{equation}
Finally, some of the papers use the Discounted Cumulative Gain(DCG) metric or the normalized version NDCG.
Assuming each user $u$ has a gain $g_{ui}$ from being recommended item $i$, then the average DCG for a list of $J$ items is defined in \autoref{eqn:dcg}, where $i_j$ is the item at position $j$ in the list, and the logarithm base is a free parameter, where $2$ is most commonly used to ensure all positions are discounted.
This metric also rewards lists that frontload relevant items.
\begin{equation}
    \label{eqn:dcg}
    DCG = \frac{1}{N} \sum\limits_{u=1}^N \sum\limits_{j = 1}^J \frac{g_{ui_j}}{log_b (j+1)}
\end{equation}
NDCG is defined in \autoref{eqn:ndcg}, where $DCG*$ is the ideal DCG, which is defined by sorting the DCG vector such that the most relevant items appear in the start of the list\cite{dcgpaper}, resulting in the biggest possible DCG value.
\begin{equation}
    \label{eqn:ndcg}
    NDCG = \frac{DCG}{DCG*}
\end{equation}
\\\\
The papers examined throughout this research employ cross validation across a number of folds, in a range of 5-10 folds.
Most of them conform to splitting the data into an 80\%/20\% split of training and test data, as seen on \autoref{tab:methodsused}.
Another interesting aspect to look into for the various papers that have been researched is which metrics they use for comparisons.
The results of this can be seen on \autoref{tab:metricsused}.

\begin{table*}[]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
    \hline
             & \textbf{MSE} & \textbf{RMSE} & \textbf{MAE} & \textbf{Precision} & \textbf{Recall} & \textbf{NDCG} & \textbf{F1} & \textbf{Hit rate} & \textbf{MAP} \\ \hline
KGAT \cite{KGAT}        &              &               &              &                    &                 & x             &             &                   &              \\ \hline
IS-UserBased \cite{GraphBasedCollaborativePaper} &              &               &              &                    &                 & x             &             &                   & x            \\ \hline
NeuMF \cite{neuMF}       &              &               &              &                    &                 & x             &             & x                 &              \\ \hline
LightGCN \cite{LightGCN}    &              &               &              &                    & x               & x             &             &                   &              \\ \hline
NGCF \cite{NGCF}         &              &               &              &                    & x               & x             &             &                   &              \\ \hline
DeepWalk \cite{DeepWalk}          &              &               &              & Implicit                & Implicit             &               & x           &                   &              \\ \hline
NMF \cite{NMF}          &              & x             &              &                    &                 &               &             &                   &              \\ \hline
CAMF(I) \cite{baltrunasCAMF}         &              &               & x            &                    &                 &               &             &                   &              \\ \hline
SVD \cite{standardMF}          & x            & x             &              &                    &                 &               &             &                   &              \\ \hline
SVD++ \cite{svd++}       &              & x             &              &                    &                 &               &             &                   &              \\ \hline
    \end{tabular}
    \caption{Metrics used}
    \label{tab:metricsused}
\end{table*}

\begin{table*}[]
    \begin{tabular}{|l|l|}
    \hline
    \textbf{}     & \textbf{Method used}                          \\ \hline
    \textbf{KGAT} & Training, validation, test (70\%, 10\%, 20\%) \\ \hline
    IS-UserBased  & 10-fold cross validation                      \\ \hline
    NeuMF         & Leave-one-out                                 \\ \hline
    LightGCN      & From original authors (including splits)      \\ \hline
    NGCF          & Training, validation, test (70\%, 10\%, 20\%) \\ \hline
    DeepWalk      & 10-fold cross validation                      \\ \hline
    NMF           & 80/20 split and 5-fold cross validation       \\ \hline
    CAMF          & 25 splits of training, test (90\%, 10\%)      \\ \hline
    SVD           & Netflix prize testing                         \\ \hline
    SVD++         & Test, validation (both 1.4 million ratings)   \\ \hline
    \end{tabular}
    \caption{Methods used}
    \label{tab:methodsused}
\end{table*}

\subsection{Summary}\label{sec:datasetsummary}
A summary of the details of the datasets can be seen in \autoref{tab:datasetstats}.
The most interesting datasets for context-aware recommendation are LDOS-CoMoDa due to the amount of contextual values, the Yahoo! dataset due to the size and inclusion of context, the MovieLens datasets due to their size and ability to extract a temporal context, as well as Frappé due to its size.
Most of the examined papers perform offline evaluation, mainly employing recall and NDCG to evaluate the algorithms, and cross-validate on the sets when testing.

\begin{table*}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|} 
    \hline
               & \#Ratings & \#Items & \#Users & \#Context variables  \\ 
    \hline
    LDOS-CoMoDa    & 2,296      & 1,232    & 121     & 12                   \\ 
    \hline
    MovieLens 1M   & 1,000,209 & 3,706    & 6,040    & 1                    \\ 
    \hline
    MovieLens 100K & 100,000   & 1,682    & 943     & 1                    \\ 
    \hline
    Frappé         & 96,203     & 957     & 4,082    & 8                    \\ 
    \hline
    InCarMusic     & 4,012      & 139     & 42      & 8                    \\ 
    \hline
    DePaulMovie    & 5,029      & 79      & 97      & 3                    \\
    \hline
    Yahoo!    & 211,231      & 11,915      & 7,642      & 2                    \\
    \hline
    \end{tabular}
    \caption{A final summary of the datasets.}
    \label{tab:datasetstats}
\end{table*}
