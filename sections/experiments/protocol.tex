\subsection{Experiments}\label{subsec:experimentprotocol}
Experiments are carried out in order to gain an understanding of baselines that employ different methods and investigate the impact of contextual data.
This section describes these experiments and the protocol used.

\subsubsection{Datasets}
Four datasets are considered for experimentation based on the investigation in \autoref{subsec:desc-of-datasets} and \autoref{sec:evaluationmetrics}: MovieLens 100k, LDOS-CoMoDa, Frappé, as well as the Yahoo! movie dataset of ratings and descriptive content information\cite{Yahoo!-movie}.
These datasets all provide contextual information, side-information, or allow for context to be inferred through the timestamps provided.
Users that have not provided at least 5 ratings are pruned for the LDOS-CoMoDa dataset.
The Frappé dataset does not provide explicit ratings, and methods that require this do not report results on this dataset.
The remaining datasets provide ratings in the range of 1 to 5.
The contextual variables and side information used is defined in \autoref{tbl:sideinfoprotocol} and \autoref{tbl:contextprotocol}.
For the time interval defined based on the timestamp there are five possible values as defined in \autoref{tbl:contextprotocol}.
For the experiments, we define these as $06.00$-$09.00$ being the morning, $09.00$-$12.00$ being noon, $12.00$-$17.00$ being afternoon, $17.00$-$22.00$ being evening, and the remaining interval $22.00$-$05.00$ being night.
KGAT uses all the side-information in \autoref{tbl:sideinfoprotocol} except for age group and gender, while for the Yahoo! dataset the age group and gender side-information is used as a substitute for contextual variables. 
\\
\begin{table*}[]\centering
    \caption{The side-information used for the experiments along with their amount of possible values.}\label{tbl:sideinfoprotocol}
    \scriptsize
    \begin{tabular}{cccc}\toprule
         & \textbf{Yahoo!} & \textbf{MovieLens} & \textbf{Frappé}\\\cmidrule{1-4}
         \multirow{8}{*}[-10pt]{Side-information} & Age group (3 values) & Age group (3 values) & Categories (32 values)\\\cmidrule{2-4}
         & Gender (2 values) & Gender (2 values) & Downloads (16 values) \\\cmidrule{2-4}
         & MPAA-rating (21 values)  &  Genre (19 values) & Developer (2809 values) \\\cmidrule{2-4}
         & Distributor (372 values)  & Release date (8 values) & Language (29 values) \\\cmidrule{2-4}
         & Genre (315 values)  &  &  \\\cmidrule{2-4}
         & DirectorID (5596 values)  &  &  \\\cmidrule{2-4}
         & ActorID (10344 values)  &  &  \\\cmidrule{2-4}
    \bottomrule
    \end{tabular}
\end{table*}
\begin{table*}[]\centering
    \caption{The contextual variables used for the experiments along with their amount of possible values.}\label{tbl:contextprotocol}
    \scriptsize
    \begin{tabular}{ccc}\toprule
         & \textbf{MovieLens} & \textbf{LDOS-CoMoDa}\\\cmidrule{1-3}
         \multirow{2}{*}[-3pt]{Contextual variables} & Day of week (7 values) & DominantEmo (5 values)\\\cmidrule{2-3}
         & Time intervals (5 values) & Physical (2 values) \\\cmidrule{2-3}
    \bottomrule
    \end{tabular}
\end{table*}

\noindent
\textbf{Cross-validation}\\
For each dataset we employ \textit{5-fold} cross-validation.
We split the data into 5 folds, and employ 4 of these as training and 1 as test.
This procedure is repeated 5 times.
The same splits are used for evaluation for each method to ensure proper comparability between the methods.

\subsection{Experimental settings}
In the following we define the evaluation metrics used, the baselines that are compared as well as the hyperparameter settings used.
\\\\
\textbf{Evaluation metrics}\\
As defined in \autoref{sec:evaluationmetrics}, the metrics used are Precision@N, Recall@N, MAP@N, RMSE and MAE.
All five metrics are reported for baselines that provide rating predictions, whereas RMSE and MAE are not reported for methods that only compute a list of top-$N$ recommendations.
For Precision@N, Recall@N and MAP@N, the $k$-value is defined as $10$.
For evaluation purposes we define a relevant item for a given user as an item in which a rating with a value of $3$ or more is observed.
\\\\
\textbf{Baselines and state-of-the-art}\\
The experiments are carried out on the following baselines:
\begin{itemize}
    \item \textbf{Random}: Using random predictions, which is done by either assigning a random rating for rating prediction, or by generating a random list for top-$N$ list recommendations.
    \item \textbf{SVD}: A standard matrix factorization approach. This is a latent factor approach, where items and users are transformed to the same latent factor space, explaining items and users on factors automatically inferred.
    \item \textbf{SVD++}: An extension of the matrix factorization latent factor approach, combining it with neighborhood models that analyze similarities between products and users.
    \item \textbf{Non-negative MF}: A variation of a matrix factorization approach for collaborative filtering, in which the matrices representing the latent factor space are subject to the non-negative constraint, i.e. $\geq$ 0.
\end{itemize}

And the following state-of-the-art methods:
\begin{itemize}
    \item \textbf{KGAT}: Considers side-information when providing recommendation by linking items with their attributes as higher-order relations in a knowledge graph. Embeddings from a node's neighbor are propagated, and an attention mechanism is used to discriminate importance of neighbors.
    \item \textbf{IS-UserBased}: A context-based recommendation approach making use of transitive associations in a bipartite graph to model and find nearest neighbor similarity to produce recommendations on a dataset that has been split on items.
    \item \textbf{LightGCN}: A graph-convolutional network approach that learns user and item embeddings by propagating them on a user-item interaction graph and uses the weighted sum of the embeddings learned at all layers as the final embedding.
    \item \textbf{NGCF}: A neural-network based approach that integrates user-item interactions into embeddings through a bipartite graph-structure.
    \item \textbf{DeepWalk + KNN}: This approach learns latent representations of nodes in a network, using information from truncated random walks. The embeddings learned through this approach are used for a $k$-nearest neighbor recommendation approach.
    \item \textbf{CAMF-C}: A context-aware extension of the SVD approach, introducing a single parameter for all contextual conditions, modeling how the rating deviates from the effect of the context.
    \item \textbf{CAMF-CI}: A variation of CAMF-C, introducing one parameter per contextual condition and item pair.
\end{itemize}
\textbf{Method setup and parameter settings}
\\
This part describes the setup used for the different methods and how the parameters were set.
For all methods that include random initialization, a seed is used to allow the methods to be rerun under the same conditions.
\begin{itemize}
    \item \textbf{KGAT}: A batch size of $1/10$ of the users in dataset is used. The learning rate used is 0.0001 for all datasets. An embedding size of 64 is used.
    \item \textbf{IS-UserBased}: For LDOS-CoMoDa the setup is $L=10$, $n=12$. For Yahoo! it is $L=8$, $n=10$. For MovieLens $L=10$, $n=20$.
    \item \textbf{LightGCN}: For all datasets, the setup is a learning rate of 0.001, 3 layers, an embedding size of 64 and a top-$N$ list of size 10 is generated. The batch size used is $1/10$ of the users in dataset.
    \item \textbf{NGCF}: For all datasets the setup is a learning rate of 0.0005, 3 layers, and an embedding size of 64 is used. The batch size used is $1/10$ of the users in dataset.
    \item \textbf{DeepWalk + KNN}: For all datasets, the embedding size used for \textit{DeepWalk} is 20. The number of neighbors used in \textit{kNN} is 10.
    \item \textbf{CAMF-C}: For all datasets, a learning rate of 0.005, a regularization coefficient of 0.2 and a batch size $1/10$ of users are used. The number of features learned in the matrix factorization is set to 10.
    \item \textbf{CAMF-CI}: For all datasets, a learning rate of 0.01, a regularization coefficient of 0.2 and a batch size $1/10$ of users are used. The number of features learned in the matrix factorization is set to 10.
\end{itemize}
