\subsection{Experiments}
Experiments are carried out in order to gain an understanding of baselines that employ different methods and investigate the impact of contextual data.
This section describes these experiments and the protocol used.

\subsubsection{Datasets}
Four datasets are considered for experimentation based on the investigation in \autoref{subsec:desc-of-datasets} and \autoref{sec:evaluationmetrics}: \textit{MovieLens 100k}, \textit{LDOS-CoMoDa}, \textit{Frappé}, as well as the \textit{Yahoo!} movie dataset of ratings and descriptive content information\cite{yahoo-movie}.
These datasets all provide contextual information, or allow for context to be inferred through the timestamps provided.
Users that have not provided at least 5 ratings are pruned for the \textit{LDOS-CoMoDa} dataset.
The \textit{Frappé} dataset does not provide explicit ratings, and methods that require this do not report results on this dataset.
The remaining datasets provide ratings in the range of $1-5$.
\\\\
\textbf{Cross-validation}\\
For each dataset we employ \textit{5-fold} cross-validation.
We split the data into 5 folds, and employ 4 of these as training and 1 as test.
This procedure is repeated 5 times.
The same splits are used for evaluation for each method to ensure proper comparability between the methods.

\subsection{Experimental settings}
In the following we define the evaluation metrics used, the baselines that are compared as well as the hyperparameter settings used.
\\\\
\textbf{Evaluation metrics}\\
As defined in \autoref{sec:evaluationmetrics}, the metrics used are \textit{precision@N}, \textit{recall@N}, \textit{map@N}, \textit{RMSE} and \textit{MAE}.
All five metrics are reported for baselines that provide rating predictions, whereas \textit{RMSE} and \textit{MAE} are not reported for methods that only compute a list of \textit{top-N} recommendations.
For \textit{precision@N}, \textit{recall@N} and \textit{map@N}, the k-value is defined as $10$.
For evaluation purposes we define a relevant item for a given user as an item in which a rating with a value of $3$ or more is observed. 
\\\\
\textbf{Baselines}\\
The experiments are carried out on the following baselines:
\begin{itemize}
    \item \textbf{Random}: Using random predictions
    \item \textbf{ItemAVG}: An average rating of the item across all users is used for predictions
    \item \textbf{TopPopular}: Always recommends the top k most popular items to a user
    \item \textbf{KGAT}: Considers side information when providing recommendation by linking items with their attributes as higher-order relations in a knowledge graph. Embeddings from a node's neighbor are recursively propagated, and an attention mechanism is used to discriminate importance of neighbors.
    \item \textbf{ItemSplitting}: A context-based recommendation approach making use of transitive associations in a bipartite graph to model and find nearest neighbor similarity to produce recommendations on a dataset that has been split on items.
    \item \textbf{NeuMF}: Replaces the inner product commonly used for matrix factorization approaches with a neural architecture combining learned embeddings from a matrix factorization and multi-layer perceptron approach.
    \item \textbf{LightGCN}: A graph-convolutional network approach that learns user and item embeddings by linearly propagating them on a user-item interaction graph and uses the weighted sum of the embeddings learned at all layers as the final embedding.
    \item \textbf{NGCF}: A neural-network based approach that integrates user-item interactions into embeddings through a bipartite graph-structure, leveraging higher-order connectivities to incorporate the collaborative signal.
    \item \textbf{DeepWalk + KNN}: This approach learns latent representations of nodes in a network, using information from truncated random walks. The embeddings learned through this approach are used for a k-nearest neighbor recommendation approach.
    \item \textbf{Non-negative MF}: Another variation of a matrix factorization approach for collaborative filtering, in which the matrices representing the latent factor space are subject to the non-negative constraint, i.e. $\geq$ 0.
    \item \textbf{CAMF-C}: A context-aware extension of the SVD approach, introducing a single parameter for all contextual conditions, modeling how the rating deviates from the effect of the context.
    \item \textbf{CAMF-CI}: A variation of CAMF-C, introducing one parameter per contextual condition and item pair.
    \item \textbf{SVD}: A standard matrix factorization approach. This is a latent factor approach, where items and users are transformed to the same latent factor space, explaining items and users on factors automatically inferred.
    \item \textbf{SVD++}: An extension of the matrix factorization latent factor approach, combining it with neighborhood models that analyze similarities between products and users.
\end{itemize}
\textbf{Parameter settings}\\
