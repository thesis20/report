\section{Discussion}\label{sec:discussion}
The following section will investigate the results acquired in \autoref{sec:experiments}.
The reported values for the different metrics will be discussed in order to gain an understanding of why the different methods perform as reported, and which results are interesting.
The comparability between different methods and datasets will also be discussed, as well as what information can be inferred in relation to the datasets.
Finally, the usefulness of context for recommendation will be examined, as well as the performance of graph based methods.


\subsection{Experimental result discussion}
\subsubsection{The value of RMSE and MAE}
\subsubsection{The trade-off between precision and recall and the F1-measure}
A trade-off between precision and recall is expected as mentioned in \autoref{sec:evaluationmetrics}.
This is due to their definitions: precision uses the length of the proposed top-$N$ list as the denominator while recall uses the number of total relevant items.
This means that if the amount of relevant items increases, the algorithm is more likely to hit a relevant item in the recommendation, thus improving overall precision.
However, an increasing number of relevant items penalizes recall, thus there is a trade-off.
As such, simply looking at one of these metrics can be misleading, as the results can be influenced in this manner.
We thus use F1 as a metric to combine these metrics and generate more comparable results between the methods.
\autoref{graph:MLPrecRecF1}, \autoref{graph:YahooPrecRecF1} and \autoref{graph:CoMoDaPrecRecF1} report the results for all of these methods.
Because of the trade-off, F1 is the most interesting metric for comparison.
\subsubsection{Comparison of MAP@K and NDCG}
MAP@K and NDCG serve a similar purpose, as defined in \autoref{sec:evaluationmetrics}.
For the predicted top-$N$ list we are not just interested in hitting relevant items, but also frontloading the list with these relevant items.
These metrics serve to measure how well a method accomplishes this.
As is evident from \autoref{graph:MAP10}, the MAP@K metric is very punishing, culminating in results of very small values approaching 0.
NDCG on the other hand, \autoref{graph:NDCG10} tends to report more easily relatable results between 0 and 1 as an indicator.
The reason for this discrepancy is in the way the calculation takes place.
NDCG looks at whether or not items are frontloaded in a top-$N$ list whenever the top-$N$ list contains relevant items, unlike MAP@K, for which any list of top-$N$ contributes to the final results, rather than just those containing relevant items.
A shortcoming of the NDCG approach is that it is easier for baselines such as random to perform well on this metric.
Since it counts frontload items only in lists that contain relevant items, it can get lucky and produce a list for a user containing frontloaded items and thus achieve a good result.
This is why it achieves comparable results to other methods that would be expected to outperform a random choice on this metric.
\subsubsection{Are the results comparable?}
\subsubsection{Inferred information on the datasets}

\subsection{Graph based and context-aware methods}
\subsubsection{How do the context-aware methods utilize context?}
CAMF 
ItemSplitting

\subsubsection{Did context matter?}
yes felipe showed this xd
sparsity problem
alleviating sprasity through knowledge
\subsubsection{Contextual discretization}
timestamps and stuff - how do we split: time of day or month or season or whatever the fuck
user attributes vs contextual
contextual information vs metainformation
\subsubsection{Timestamp as a context variable}
Often the only available context variable in larger datasets.
Can be extracted and used.
However, an assumption made when doing so is that the rating was given by the user at the time of finishing consumption of the item??
As described in \autoref{subsec:desc-of-datasets}, \cite{COMODA2013} defines that some of the most infulential context variables are the dominant emotions during the watching of the movie as well as the final emotion felt.
As such, we can infer that ratings provided shortly after watching does a better job of reflecting the contextual influence on the rating.
However, this cannot be guaranteed as some users might rate a batch of items in quick succession, or even rate a single item after a substantial amount of time has passed since consumption.
This means the data could lose some of the contextual impact if timestamp is the only context variable that is available or inferable.  
%TODO Ref til der vi beskriver hivlke context vcariabler vi kigger p√• til comoda
\subsubsection{The performance of graph based methods}
they suck lol


