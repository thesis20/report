\section{Conclusion}\label{sec:conclusion}
Throughout this paper, we have looked into various metrics used for evaluating RS. What we found was that there is no consistent set of metrics used for evaluation, but the most commonly used are NDCG and Recall.
We found that utilizing bipartite graphs is a popular and intuitive way to handle information in the RS, but in the investigated metrics, graph-based methods are often outperformed by traditional RS such as SVD.
The cause of this is speculative but may be because methods like SVD make use of explicit rating information, where most of the investigated graph-based methods use implicit ratings.
\\\\
Based on the results from the experiment, there is not conclusive evidence that the CARS methods perform better than methods that do not utilize context.
From the knowledge acquired throughout this paper, we propose that instead of only looking into context as information about the interaction, it could be beneficial to expand the model to include side-information about the items or users in addition to the context.
This could allow the system to identify otherwise unseen trends, such as a user liking a specific movie genre in a certain context, rather than just looking at which specific movies they like.
Additionally, adding information about the users may help connect like-minded users in scenarios where the current user has not yet rated anything in their given context.
\\\\
The results showed that the performance of the examined methods vary based on both the dataset used and chosen metric, with no clear superior method.
